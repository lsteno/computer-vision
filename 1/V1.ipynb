{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f105768-13e4-457a-a607-ea0d4345ce52",
   "metadata": {},
   "source": [
    "**Assignment 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff282e81-bd81-40e8-bfa5-1f7c72ae67b1",
   "metadata": {},
   "source": [
    "**Exercise 1.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5988b57-4dd2-4a6e-bec4-775fab2c1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4b6bdd-f44a-4435-9547-3df2f1662fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf20073-fbec-4331-b1e9-3c5186b83182",
   "metadata": {},
   "source": [
    "a) Loading the CIFAR10 dataset in 'trainset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ccd22a-4656-465c-a478-e996af9310c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to convert PIL images to tensors.\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the training set of CIFAR10.\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d349b-7e04-4d36-81ea-68ec966312e0",
   "metadata": {},
   "source": [
    "b) Print the number of samples and the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc6b138c-bdc0-43bc-b935-1888741d974a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 50000\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples:\", len(trainset))\n",
    "print(\"Number of classes:\", len(trainset.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4de5ea-85b4-4f66-875d-5fc49c21e0ba",
   "metadata": {},
   "source": [
    "c) Get an example image and print its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb537c1b-cff3-4c4c-8b36-e89b848eb462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of an image: torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "image, label = trainset[1234]\n",
    "print(\"Shape of an image:\", image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1768941-4d84-47dd-9278-83d24514f4b1",
   "metadata": {},
   "source": [
    "**Exercise 1.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea4352-efe7-4112-8902-a82437a3316f",
   "metadata": {},
   "source": [
    "a) Print the number of samples per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d498e0-d196-4443-9707-b8757f1c02e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples per category:\n",
      "frog: 5000\n",
      "truck: 5000\n",
      "deer: 5000\n",
      "automobile: 5000\n",
      "bird: 5000\n",
      "horse: 5000\n",
      "ship: 5000\n",
      "cat: 5000\n",
      "dog: 5000\n",
      "airplane: 5000\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Extract labels from the dataset.\n",
    "labels = [label for (_, label) in trainset]\n",
    "\n",
    "# Count samples for each category using Counter.\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(\"Number of samples per category:\")\n",
    "for label_idx, count in label_counts.items():\n",
    "    print(f\"{trainset.classes[label_idx]}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10862a9-1161-468d-8a45-115a17ff8591",
   "metadata": {},
   "source": [
    "b) Plot the number of samples per category using a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7100ee0-3b13-4a97-aefd-5f0ac11a3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting.\n",
    "categories = trainset.classes\n",
    "counts = [label_counts[i] for i in range(len(categories))]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(categories, counts, color='skyblue')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Number of Samples per Category in CIFAR-10')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6251bba-f658-4e25-a8bb-eadae91bc26a",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset is balanced; each of the 10 categories has the same number of samples (5,000 images in the training set). There are no majoritarian classes, so you do not have to worry about class imbalance affecting the analysis or the training of your models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799eab15-c929-4864-bd30-628d161c462b",
   "metadata": {},
   "source": [
    "**Exercise 1.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b7f289-8d54-45d2-b79e-9699e9a6910e",
   "metadata": {},
   "source": [
    "Create a figure with n x 4 sub-plots. The value of 'n' depends on the number of categories present in the dataset. As the title of each row in your figure, indicate the category it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d51a3a1-b5a6-4f7a-9f18-5ecfbeef259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset.\n",
    "num_classes = len(trainset.classes)\n",
    "\n",
    "# Collect 4 images per category.\n",
    "images_by_class = {i: [] for i in range(num_classes)}\n",
    "for img, label in trainset:\n",
    "    if len(images_by_class[label]) < 4:\n",
    "        images_by_class[label].append(img)\n",
    "    # Stop when we have 4 images for each category.\n",
    "    if all(len(img_list) == 4 for img_list in images_by_class.values()):\n",
    "        break\n",
    "\n",
    "# Create a figure with n rows and 4 columns.\n",
    "fig, axes = plt.subplots(num_classes, 4, figsize=(12, 2.5 * num_classes))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "# Loop through each category (row) and each sample (column).\n",
    "for i in range(num_classes):\n",
    "    for j in range(4):\n",
    "        ax = axes[i, j]\n",
    "        # Convert tensor (C, H, W) to numpy array (H, W, C) for display.\n",
    "        img_np = images_by_class[i][j].numpy().transpose(1, 2, 0)\n",
    "        ax.imshow(img_np)\n",
    "        ax.axis('off')\n",
    "    # Use the left-most subplot in each row to display the category name.\n",
    "    axes[i, 0].set_ylabel(trainset.classes[i], rotation=0, labelpad=40, fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d0b3f-d7d7-486c-8e76-a34c9fc2cd9a",
   "metadata": {},
   "source": [
    "**Exercise 1.4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0c581-03b9-41c8-b726-685c461f10a7",
   "metadata": {},
   "source": [
    "Extract RGB values from each image in your dataset as three seperate lists(one per channel). Each list should have 8 values. To do so, you can compute the histogram of each channel with 8 bins. Then you have to concatenate the values of all the three channels together resulting in a feature vector of size 24. This feature vector is the descriptor of an image in your dataset. You will have to do this for all the images present in your dataset in order to get the overall RGB descriptor which will be of size (n,24). Here 'n' depends on the number of samples present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079af4e4-26c0-49d7-9ebd-0fd00e8b7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the feature vectors (descriptors) for each image.\n",
    "descriptors = []\n",
    "\n",
    "# Iterate over all images in the dataset.\n",
    "for img, _ in trainset:\n",
    "    # 'img' is a tensor of shape (3, H, W) with values in [0, 1].\n",
    "    img_np = img.numpy()  # Convert to NumPy array.\n",
    "    feature_vector = []\n",
    "    \n",
    "    # Loop over each channel: 0: Red, 1: Green, 2: Blue.\n",
    "    for channel in img_np:\n",
    "        # Compute the histogram with 8 bins for the current channel.\n",
    "        hist, _ = np.histogram(channel, bins=8, range=(0, 1))\n",
    "        # Append the histogram counts to the feature vector.\n",
    "        feature_vector.extend(hist.tolist())\n",
    "    \n",
    "    # The resulting feature_vector has 24 values (8 per channel).\n",
    "    descriptors.append(feature_vector)\n",
    "\n",
    "# Convert the list of descriptors into a NumPy array of shape (n, 24).\n",
    "descriptors = np.array(descriptors)\n",
    "print(\"Shape of overall RGB descriptor:\", descriptors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02b3e4-f992-4f4b-9bc1-fafba383850a",
   "metadata": {},
   "source": [
    "**Exercise 1.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6724fa6-b621-4b2a-8110-2f260892a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute descriptors and collect labels.\n",
    "# Each descriptor is a 24-dimensional vector obtained by computing 8-bin histograms for each channel.\n",
    "descriptors = []\n",
    "labels = []\n",
    "for img, label in trainset:\n",
    "    img_np = img.numpy()  # shape: (3, H, W)\n",
    "    feature_vector = []\n",
    "    for channel in img_np:\n",
    "        # Compute an 8-bin histogram for the current channel over the range [0, 1].\n",
    "        hist, _ = np.histogram(channel, bins=8, range=(0, 1))\n",
    "        feature_vector.extend(hist.tolist())\n",
    "    descriptors.append(feature_vector)\n",
    "    labels.append(label)\n",
    "\n",
    "descriptors = np.array(descriptors)  # overall descriptor array of shape (n, 24)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Compute the mean descriptor for each class.\n",
    "# CIFAR-10 has 10 classes.\n",
    "class_names = trainset.classes  # List of class names.\n",
    "mean_descriptors = {}\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Select all descriptors corresponding to class i.\n",
    "    class_descriptors = descriptors[labels == i]\n",
    "    # Compute the mean descriptor (a vector of length 24) for this class.\n",
    "    mean_descriptor = np.mean(class_descriptors, axis=0)\n",
    "    mean_descriptors[class_name] = mean_descriptor\n",
    "\n",
    "# Create a pandas DataFrame from the mean descriptors.\n",
    "# Rows correspond to classes and columns correspond to the 24 feature values.\n",
    "df = pd.DataFrame(mean_descriptors).T\n",
    "df.columns = [f'bin_{i+1}' for i in range(df.shape[1])]\n",
    "\n",
    "# Compute the inter-class correlation.\n",
    "# Since df has classes as rows, we compute the correlation among rows by transposing it.\n",
    "inter_class_corr = df.T.corr()\n",
    "\n",
    "print(\"Inter-class correlation matrix:\")\n",
    "print(inter_class_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cfd512-cca4-4e1c-8289-734ffdd3e1a7",
   "metadata": {},
   "source": [
    "c) Compute the Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbacaec-4707-4111-99c2-3add3892b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Assuming 'descriptors' is a NumPy array of shape (n_samples, 24)\n",
    "# and 'labels' is a NumPy array of the corresponding CIFAR-10 class labels.\n",
    "sil_score = silhouette_score(descriptors, labels, metric='euclidean')\n",
    "print(\"Silhouette Score for the extracted descriptors:\", sil_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb38f1-7248-4875-9e0f-826a2dd125e1",
   "metadata": {},
   "source": [
    "Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9310bd-23eb-4623-9b0c-b7a04f9d741a",
   "metadata": {},
   "source": [
    "1. Does Intra-class correlation score/coefficient help you assess the degree of similarity among the samples of a category?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aca020-a012-4b17-8c03-9d693d813398",
   "metadata": {},
   "source": [
    "Yes, the intra-class correlation score is a useful measure for assessing how similar the samples within a single category are. A higher intra-class correlation indicates that the samples share common characteristics and are clustered tightly together, which means the feature descriptor is capturing consistent traits of that category. Conversely, a lower intra-class correlation would suggest that the samples vary significantly from one another, potentially indicating that the features are not robust or discriminative enough for that category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1827838-126e-45a5-a2e5-8bffacbbef40",
   "metadata": {},
   "source": [
    "What can you deduce from the Inter-class correlation and Silhouette score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac47d5f-77a7-4edb-9a71-c15db9ccab0b",
   "metadata": {},
   "source": [
    "The inter-class correlation reveals how similar the mean descriptors of different categories are. If the inter-class correlation is high, it suggests that the classes share similar feature characteristics, which could lead to overlapping clusters and difficulties in distinguishing between them. On the other hand, a low inter-class correlation implies that the classes are well separated in the feature space. The silhouette score complements this by evaluating the overall compactness and separation of the clusters. A high silhouette score indicates that the samples are well-clustered within their respective categories and are well separated from other categories, while a low or negative silhouette score would point to overlapping clusters. Together, these metrics provide insights into both the consistency within classes and the distinctiveness between classes, which are critical for effective classification and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1d422-459b-4056-8449-f4557f690a61",
   "metadata": {},
   "source": [
    "**Exercise 1.6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433551d9-5d06-45ce-a475-139f9604edfd",
   "metadata": {},
   "source": [
    "a) Rely on the first 2 principal components to plot the samples of your dataset. Use one color per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf0d2c-e1c8-4ce8-83d2-93e443bd8cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce to 2 dimensions.\n",
    "pca_2d = PCA(n_components=2)\n",
    "descriptors_2d = pca_2d.fit_transform(descriptors)\n",
    "\n",
    "# Define colors for 10 classes.\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'orange', 'purple', 'brown']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, class_name in enumerate(trainset.classes):\n",
    "    idx = (labels == i)\n",
    "    plt.scatter(descriptors_2d[idx, 0], descriptors_2d[idx, 1],\n",
    "                color=colors[i], label=class_name, alpha=0.5)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"2D PCA of CIFAR-10 Descriptors\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa7223-3a66-45a9-80bf-70fe4fba122d",
   "metadata": {},
   "source": [
    "b) Rely on the first 3 principal components to create a 3D plot. Use one color per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8bc102-ee4f-4aed-9f29-06d9148cf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # Needed for 3D plotting\n",
    "\n",
    "# Apply PCA to reduce to 3 dimensions.\n",
    "pca_3d = PCA(n_components=3)\n",
    "descriptors_3d = pca_3d.fit_transform(descriptors)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i, class_name in enumerate(trainset.classes):\n",
    "    idx = (labels == i)\n",
    "    ax.scatter(descriptors_3d[idx, 0], descriptors_3d[idx, 1], descriptors_3d[idx, 2],\n",
    "               color=colors[i], label=class_name, alpha=0.5)\n",
    "ax.set_xlabel(\"PC 1\")\n",
    "ax.set_ylabel(\"PC 2\")\n",
    "ax.set_zlabel(\"PC 3\")\n",
    "ax.set_title(\"3D PCA of CIFAR-10 Descriptors\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d4097-4e26-4810-8df1-b3818e3f0905",
   "metadata": {},
   "source": [
    "**Exercise 1.7 – Reflection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f605b4a-25eb-4591-bf6f-805bb0eb6121",
   "metadata": {},
   "source": [
    "a) Will you obtain the same visualisation in the feature space for different extracted features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3cfe5a-e399-4fdf-88d0-154a07959737",
   "metadata": {},
   "source": [
    "No, different feature extraction methods capture different aspects of the data. For instance, using simple RGB histograms versus more advanced features (like SIFT, HOG, or deep features) will result in different distributions in the feature space. As a consequence, the PCA visualizations will likely differ based on the extracted features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c674bb5-b49f-4fe6-a8bd-fd8179ab44f4",
   "metadata": {},
   "source": [
    "b) Are the classes distinguishable on the feature space when relying on PCA over RGB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b246efb7-bf54-446f-bea1-054728111468",
   "metadata": {},
   "source": [
    "Using PCA over basic RGB histograms often does not yield a clearly separable clustering of classes. The RGB descriptors may not capture the higher-level semantics of the images, so some classes might overlap, resulting in less distinguishable clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d3c8c-fe03-4ce8-ac9c-01f36f98ee60",
   "metadata": {},
   "source": [
    "c) What other visualisation could you include to better describe your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0c418-c6b9-4413-b6c2-0853a6f7baeb",
   "metadata": {},
   "source": [
    "Other visualization techniques such as t-SNE or UMAP can provide non-linear dimensionality reduction that often reveals better cluster separation. Additionally, pair plots or dendrograms from hierarchical clustering could help visualize the relationships between samples and classes more effectively.\n",
    "\n",
    "This completes the solution for the PCA visualization tasks and the reflection questions for your assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
